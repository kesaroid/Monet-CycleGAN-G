{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n### In this notebook we use [CycleGAN](https://arxiv.org/abs/1703.10593) to convert Monet Paintings to Realistic Photos using [Monet2Photo Dataset](https://www.kaggle.com/balraj98/monet2photo).","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://junyanz.github.io/CycleGAN/images/teaser.jpg\" width=\"1000\" height=\"900\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/pdf/1703.10593.pdf\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [C. Jun-Yan Zhu et al.]</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Libraries 📚⬇","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport time, datetime\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nfrom scipy import signal\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Settings ⚙️","metadata":{}},{"cell_type":"code","source":"# path to pre-trained models\npretrained_model_path = \"../input/cyclegan-translating-paintings-to-photos-pytorch/saved_models\"\n# epoch to start training from\nepoch_start = 8\n# number of epochs of training\nn_epochs = 16\n# name of the dataset\ndataset_path = \"../input/monet2photo\"\n# size of the batches\"\nbatch_size = 4\n# adam: learning rate\nlr = 0.00012\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# epoch from which to start lr decay\ndecay_epoch = 1\n# number of cpu threads to use during batch generation\nn_workers = 8\n# size of image height\nimg_height = 256\n# size of image width\nimg_width = 256\n# number of image channels\nchannels = 3\n# interval between saving generator outputs\nsample_interval = 100\n# interval between saving model checkpoints\ncheckpoint_interval = -1\n# number of residual blocks in generator\nn_residual_blocks = 9\n# cycle loss weight\nlambda_cyc = 10.0\n# identity loss weight\nlambda_id = 5.0\n# Development / Debug Mode\ndebug_mode = False\n\n# Create images and checkpoint directories\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Utilities","metadata":{}},{"cell_type":"code","source":"def to_rgb(image):\n    rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image\n\n\nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset Class","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, f\"{mode}A\") + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, f\"{mode}B\") + \"/*.*\"))\n        if debug_mode:\n            self.files_A = self.files_A[:100]\n            self.files_B = self.files_B[:100]\n\n    def __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n\n        if self.unaligned:\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        # Convert grayscale images to rgb\n        if image_A.mode != \"RGB\":\n            image_A = to_rgb(image_A)\n        if image_B.mode != \"RGB\":\n            image_B = to_rgb(image_B)\n\n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {\"A\": item_A, \"B\": item_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Train/Test Dataloaders","metadata":{}},{"cell_type":"code","source":"# Image transformations\ntransforms_ = [\n    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n    transforms.RandomCrop((img_height, img_width)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\n# Training data loader\ntrain_dataloader = DataLoader(\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_, unaligned=True),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_workers,\n)\n# Test data loader\ntest_dataloader = DataLoader(\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_, unaligned=True, mode=\"test\"),\n    batch_size=1,\n    shuffle=True,\n    num_workers=1,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/700/1*_KxtJIVtZjVaxxl-Yl1vJg.png\" width=\"900\" height=\"900\"/>\n<h4></h4>\n<h4><center>Image Source:  <a href=\"https://arxiv.org/pdf/1703.10593.pdf\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [C. Jun-Yan Zhu et al.]</a></center></h4>","metadata":{}},{"cell_type":"markdown","source":"### Define Model Classes","metadata":{}},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features //= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train CycleGAN","metadata":{}},{"cell_type":"code","source":"# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()\n\ncuda = torch.cuda.is_available()\n\ninput_shape = (channels, img_height, img_width)\n\n# Initialize generator and discriminator\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\nD_A = Discriminator(input_shape)\nD_B = Discriminator(input_shape)\n\nif cuda:\n    G_AB = G_AB.cuda()\n    G_BA = G_BA.cuda()\n    D_A = D_A.cuda()\n    D_B = D_B.cuda()\n    criterion_GAN.cuda()\n    criterion_cycle.cuda()\n    criterion_identity.cuda()\n\nif epoch_start != 0:\n    # Load pretrained models\n    G_AB.load_state_dict(torch.load(f\"{pretrained_model_path}/G_AB.pth\"))\n    G_BA.load_state_dict(torch.load(f\"{pretrained_model_path}/G_BA.pth\"))\n    D_A.load_state_dict(torch.load(f\"{pretrained_model_path}/D_A.pth\"))\n    D_B.load_state_dict(torch.load(f\"{pretrained_model_path}/D_B.pth\"))\nelse:\n    # Initialize weights\n    G_AB.apply(weights_init_normal)\n    G_BA.apply(weights_init_normal)\n    D_A.apply(weights_init_normal)\n    D_B.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(\n    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n)\noptimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n\n# Buffers of previously generated samples\nfake_A_buffer = ReplayBuffer()\nfake_B_buffer = ReplayBuffer()\n\ntrain_counter = []\ntrain_losses_gen, train_losses_id, train_losses_gan, train_losses_cyc = [], [], [], []\ntrain_losses_disc, train_losses_disc_a, train_losses_disc_b = [], [], []\n\ntest_counter = [2*idx*len(train_dataloader.dataset) for idx in range(epoch_start+1, n_epochs+1)]\ntest_losses_gen, test_losses_disc = [], []\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epoch_start, n_epochs):\n    \n    #### Training\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, batch in enumerate(tqdm_bar):\n\n        # Set model input\n        real_A = Variable(batch[\"A\"].type(Tensor))\n        real_B = Variable(batch[\"B\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n\n        ### Train Generators\n        G_AB.train()\n        G_BA.train()\n        optimizer_G.zero_grad()\n        # Identity loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) / 2\n        # GAN loss\n        fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n        # Cycle loss\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n        # Total loss\n        loss_G = lambda_id * loss_identity + loss_GAN + lambda_cyc * loss_cycle\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator-A\n        D_A.train()\n        optimizer_D_A.zero_grad()\n        # Real loss\n        loss_real = criterion_GAN(D_A(real_A), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n        # Total loss\n        loss_D_A = (loss_real + loss_fake) / 2\n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n        ### Train Discriminator-B\n        D_B.train()\n        optimizer_D_B.zero_grad()\n        # Real loss\n        loss_real = criterion_GAN(D_B(real_B), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n        # Total loss\n        loss_D_B = (loss_real + loss_fake) / 2\n        loss_D_B.backward()\n        optimizer_D_B.step()\n        loss_D = (loss_D_A + loss_D_B) / 2\n\n        ### Log Progress\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n        train_counter.append(2*(batch_idx*batch_size + real_A.size(0) + epoch*len(train_dataloader.dataset)))\n        train_losses_gen.append(loss_G.item()); train_losses_id.append(loss_identity.item()); train_losses_gan.append(loss_GAN.item()); train_losses_cyc.append(loss_cycle.item())\n        train_losses_disc.append(loss_D.item()); train_losses_disc_a.append(loss_D_A.item()); train_losses_disc_b.append(loss_D_B.item())\n        tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\n                            Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\n\n    #### Testing\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, batch in enumerate(tqdm_bar):\n\n        # Set model input\n        real_A = Variable(batch[\"A\"].type(Tensor))\n        real_B = Variable(batch[\"B\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n\n        ### Test Generators\n        G_AB.eval()\n        G_BA.eval()\n        # Identity loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) / 2\n        # GAN loss\n        fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n        # Cycle loss\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n        # Total loss\n        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n\n        ### Test Discriminator-A\n        D_A.eval()\n        # Real loss\n        loss_real = criterion_GAN(D_A(real_A), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n        # Total loss\n        loss_D_A = (loss_real + loss_fake) / 2\n\n        ### Test Discriminator-B\n        D_B.eval()\n        # Real loss\n        loss_real = criterion_GAN(D_B(real_B), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n        # Total loss\n        loss_D_B = (loss_real + loss_fake) / 2\n        loss_D = (loss_D_A + loss_D_B) / 2\n        \n        ### Log Progress\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n        tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\n                            Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\n        \n        # If at sample interval save image\n        if random.uniform(0,1)<0.4:\n            # Arrange images along x-axis\n            real_A = make_grid(real_A, nrow=1, normalize=True)\n            real_B = make_grid(real_B, nrow=1, normalize=True)\n            fake_A = make_grid(fake_A, nrow=1, normalize=True)\n            fake_B = make_grid(fake_B, nrow=1, normalize=True)\n            # Arange images along y-axis\n            image_grid = torch.cat((real_A, fake_B, real_B, fake_A), -1)\n            save_image(image_grid, f\"images/{batch_idx}.png\", normalize=False)\n\n    test_losses_gen.append(loss_gen/len(test_dataloader))\n    test_losses_disc.append(loss_disc/len(test_dataloader))\n\n    # Save model checkpoints\n    if np.argmin(test_losses_gen) == len(test_losses_gen)-1:\n        # Save model checkpoints\n        torch.save(G_AB.state_dict(), \"saved_models/G_AB.pth\")\n        torch.save(G_BA.state_dict(), \"saved_models/G_BA.pth\")\n        torch.save(D_A.state_dict(), \"saved_models/D_A.pth\")\n        torch.save(D_B.state_dict(), \"saved_models/D_B.pth\")\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gen, mode='lines', name='Train Gen Loss (Loss_G)'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_id, mode='lines', name='Train Gen Identity Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gan, mode='lines', name='Train Gen GAN Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_cyc, mode='lines', name='Train Gen Cyclic Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_gen, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Gen Loss (Loss_G)'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen (A+B)\",\n    yaxis_title=\"Generator Losses\"),\nplotly.offline.plot(fig, filename = 'plotly_gen_losses.html')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc, mode='lines', name='Train Disc Loss (Loss_D)'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_a, mode='lines', name='Train Disc-A Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_b, mode='lines', name='Train Disc-B Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_disc, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Disc Loss (Loss_G)'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen (A+B)\",\n    yaxis_title=\"Discriminator Losses\"),\nplotly.offline.plot(fig, filename = 'plotly_disc_losses.html')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Work in Progress ...\n\n### Note: The order of the below saved images are (from left-to-right): \n\n* Real Monet Painting\n* Fake Photo (from Gen-AB)\n* Real Photo\n* Fake Monet Painting (from Gen-BA)","metadata":{}}]}